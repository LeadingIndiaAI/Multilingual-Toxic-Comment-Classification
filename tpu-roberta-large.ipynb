{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 192  #Reduced for quicker execution\n",
    "LR = 1e-5\n",
    "BATCH_SIZE = 16 # per TPU core\n",
    "TOTAL_STEPS_STAGE1 = 300\n",
    "VALIDATE_EVERY_STAGE1 = 100\n",
    "TOTAL_STEPS_STAGE2 = 200\n",
    "VALIDATE_EVERY_STAGE2 = 100\n",
    "\n",
    "PRETRAINED_MODEL = 'jplu/tf-xlm-roberta-large'\n",
    "D = '/kaggle/input/jigsaw-multilingual-toxic-comment-classification/'\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "import transformers\n",
    "from transformers import TFAutoModel, AutoTokenizer\n",
    "import logging\n",
    "# no extensive logging \n",
    "logging.getLogger().setLevel(logging.NOTSET)\n",
    "\n",
    "AUTO = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on TPU  grpc://10.0.0.2:8470\n",
      "REPLICAS:  8\n"
     ]
    }
   ],
   "source": [
    "def connect_to_TPU():\n",
    "    \"\"\"Detect hardware, return appropriate distribution strategy\"\"\"\n",
    "    try:\n",
    "        # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
    "        # set: this is always the case on Kaggle.\n",
    "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "        print('Running on TPU ', tpu.master())\n",
    "    except ValueError:\n",
    "        tpu = None\n",
    "\n",
    "    if tpu:\n",
    "        tf.config.experimental_connect_to_cluster(tpu)\n",
    "        tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "    else:\n",
    "        # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "        strategy = tf.distribute.get_strategy()\n",
    "\n",
    "    global_batch_size = BATCH_SIZE * strategy.num_replicas_in_sync\n",
    "\n",
    "    return tpu, strategy, global_batch_size\n",
    "\n",
    "\n",
    "tpu, strategy, global_batch_size = connect_to_TPU()\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(D+'jigsaw-toxic-comment-train.csv')\n",
    "val_df = pd.read_csv(D+'validation.csv')\n",
    "test_df = pd.read_csv(D+'test.csv')\n",
    "sub_df = pd.read_csv(D+'sample_submission.csv')\n",
    "\n",
    "# subsample the train dataframe to 50%-50%\n",
    "train_df = pd.concat([\n",
    "    train_df.query('toxic==1'),\n",
    "    train_df.query('toxic==0').sample(sum(train_df.toxic),random_state=42)\n",
    "])\n",
    "# shufle it just to make sure\n",
    "train_df = train_df.sample(frac=1, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 29s, sys: 521 ms, total: 1min 30s\n",
      "Wall time: 1min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def regular_encode(texts, tokenizer, maxlen=512):\n",
    "    enc_di = tokenizer.batch_encode_plus(\n",
    "        texts, \n",
    "        return_attention_masks=False, \n",
    "        return_token_type_ids=False,\n",
    "        pad_to_max_length=True,\n",
    "        max_length=maxlen\n",
    "    )\n",
    "    \n",
    "    return np.array(enc_di['input_ids'])\n",
    "    \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL)\n",
    "X_train = regular_encode(train_df.comment_text.values, tokenizer, maxlen=MAX_LEN)\n",
    "X_val = regular_encode(val_df.comment_text.values, tokenizer, maxlen=MAX_LEN)\n",
    "X_test = regular_encode(test_df.content.values, tokenizer, maxlen=MAX_LEN)\n",
    "\n",
    "y_train = train_df.toxic.values.reshape(-1,1)\n",
    "y_val = val_df.toxic.values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dist_dataset(X, y=None, training=False):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "\n",
    "    ### Add y if present ###\n",
    "    if y is not None:\n",
    "        dataset_y = tf.data.Dataset.from_tensor_slices(y)\n",
    "        dataset = tf.data.Dataset.zip((dataset, dataset_y))\n",
    "        \n",
    "    ### Repeat if training ###\n",
    "    if training:\n",
    "        dataset = dataset.shuffle(len(X)).repeat()\n",
    "\n",
    "    dataset = dataset.batch(global_batch_size).prefetch(AUTO)\n",
    "\n",
    "    ### make it distributed  ###\n",
    "    dist_dataset = strategy.experimental_distribute_dataset(dataset)\n",
    "\n",
    "    return dist_dataset\n",
    "    \n",
    "    \n",
    "train_dist_dataset = create_dist_dataset(X_train, y_train, True)\n",
    "val_dist_dataset   = create_dist_dataset(X_val)\n",
    "test_dist_dataset  = create_dist_dataset(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_word_ids (InputLayer)  [(None, 192)]             0         \n",
      "_________________________________________________________________\n",
      "tf_roberta_model (TFRobertaM ((None, 192, 1024), (None 559890432 \n",
      "_________________________________________________________________\n",
      "tf_op_layer_strided_slice (T [(None, 1024)]            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 1025      \n",
      "=================================================================\n",
      "Total params: 559,891,457\n",
      "Trainable params: 559,891,457\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "CPU times: user 1min 54s, sys: 37.4 s, total: 2min 31s\n",
      "Wall time: 2min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def create_model_and_optimizer():\n",
    "    with strategy.scope():\n",
    "        transformer_layer = TFAutoModel.from_pretrained(PRETRAINED_MODEL)                \n",
    "        model = build_model(transformer_layer)\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=LR, epsilon=1e-08)\n",
    "    return model, optimizer\n",
    "\n",
    "\n",
    "def build_model(transformer):\n",
    "    inp = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    # Huggingface transformers have multiple outputs, embeddings are the first one\n",
    "    # let's slice out the first position, the paper says its not worse than pooling\n",
    "    x = transformer(inp)[0][:, 0, :]  \n",
    "    out = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs=[inp], outputs=[out])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "model, optimizer = create_model_and_optimizer()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_losses_and_metrics():\n",
    "    with strategy.scope():\n",
    "        loss_object = tf.keras.losses.BinaryCrossentropy(\n",
    "            reduction=tf.keras.losses.Reduction.NONE, from_logits=False)\n",
    "\n",
    "        def compute_loss(labels, predictions):\n",
    "            per_example_loss = loss_object(labels, predictions)\n",
    "            loss = tf.nn.compute_average_loss(\n",
    "                per_example_loss, global_batch_size = global_batch_size)\n",
    "            return loss\n",
    "\n",
    "        train_accuracy_metric = tf.keras.metrics.AUC(name='training_AUC')\n",
    "\n",
    "    return compute_loss, train_accuracy_metric\n",
    "\n",
    "\n",
    "\n",
    "def train(train_dist_dataset, val_dist_dataset=None, y_val=None,\n",
    "          total_steps=5000, validate_every=500):\n",
    "    step = 0\n",
    "    ### Training lopp ###\n",
    "    for tensor in train_dist_dataset:\n",
    "        distributed_train_step(tensor) \n",
    "        step+=1\n",
    "\n",
    "        if (step % validate_every == 0):   \n",
    "            ### Print train metrics ###  \n",
    "            train_metric = train_accuracy_metric.result().numpy()\n",
    "            print(\"Step %d, train AUC: %.5f\" % (step, train_metric))   \n",
    "            \n",
    "            ### Test loop with exact AUC ###\n",
    "            if val_dist_dataset:\n",
    "                val_metric = roc_auc_score(y_val, predict(val_dist_dataset))\n",
    "                print(\"     validation AUC: %.5f\" %  val_metric)   \n",
    "\n",
    "            ### Reset (train) metrics ###\n",
    "            train_accuracy_metric.reset_states()\n",
    "            \n",
    "        if step  == total_steps:\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def distributed_train_step(data):\n",
    "    strategy.experimental_run_v2(train_step, args=(data,))\n",
    "\n",
    "def train_step(inputs):\n",
    "    features, labels = inputs\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(features, training=True)\n",
    "        loss = compute_loss(labels, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_accuracy_metric.update_state(labels, predictions)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predict(dataset):  \n",
    "    predictions = []\n",
    "    for tensor in dataset:\n",
    "        predictions.append(distributed_prediction_step(tensor))\n",
    "    ### stack replicas and batches\n",
    "    predictions = np.vstack(list(map(np.vstack,predictions)))\n",
    "    return predictions\n",
    "\n",
    "@tf.function\n",
    "def distributed_prediction_step(data):\n",
    "    predictions = strategy.experimental_run_v2(prediction_step, args=(data,))\n",
    "    return strategy.experimental_local_results(predictions)\n",
    "\n",
    "def prediction_step(inputs):\n",
    "    features = inputs  # note datasets used in prediction do not have labels\n",
    "    predictions = model(features, training=False)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "compute_loss, train_accuracy_metric = define_losses_and_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:430: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 256002048 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:430: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 256002048 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100, train AUC: 0.85600\n",
      "     validation AUC: 0.90543\n",
      "Step 200, train AUC: 0.96966\n",
      "     validation AUC: 0.91020\n",
      "Step 300, train AUC: 0.97533\n",
      "     validation AUC: 0.90138\n",
      "CPU times: user 1min 17s, sys: 7.79 s, total: 1min 25s\n",
      "Wall time: 5min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train(train_dist_dataset, val_dist_dataset, y_val,\n",
    "      TOTAL_STEPS_STAGE1, VALIDATE_EVERY_STAGE1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100, train AUC: 0.92781\n",
      "Step 200, train AUC: 0.96575\n",
      "CPU times: user 8.76 s, sys: 1.78 s, total: 10.5 s\n",
      "Wall time: 1min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# make a new dataset for training with the validation data \n",
    "# with targets, shuffling and repeating\n",
    "val_dist_dataset_4_training = create_dist_dataset(X_val, y_val, training=True)\n",
    "\n",
    "# train again\n",
    "train(val_dist_dataset_4_training,\n",
    "      total_steps = TOTAL_STEPS_STAGE2, \n",
    "      validate_every = VALIDATE_EVERY_STAGE2)  # not validating but printing now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.5 s, sys: 5.35 s, total: 29.8 s\n",
      "Wall time: 1min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sub_df['toxic'] = predict(test_dist_dataset)[:,0]\n",
    "sub_df.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
